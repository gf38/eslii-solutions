\section{Overview of Supervised Learning}

%2.1
\begin{sol}
Since
\[
\hat{k}\triangleq\argmin_k\|t_k-\hat{y}\|=\argmin_k\|t_k-\hat{y}\|^2=\argmin_k\sum_{i\ne k}\hat{y}_i^2+(1-\hat{y}_k)^2
\]
and elements of $\hat{y}$ sum to one, so if $\hat{y}_k$ goes larger, both $\sum_{i\ne k}\hat{y}_i^2$ and $(1-\hat{y}_k)^2$ will get smaller, which means $\hat{k}=\argmax_k\hat{y}_i$ 
\end{sol}

%2.2
\begin{sol}
According to the descriptions at page 16 and 17, the boundary is $\{x\mid \Pr(C=\mathrm{Orange}\vert X=x) =\Pr(C=\mathrm{Blue}\vert X=x)\}$. Suppose the $\{m_k^C\}_{k=1}^{10}$ are known, and introduce a latent variable $h$ for the index of the selected $m^C$. We have
\begin{align*}
\Pr(C\vert X) =& \frac{\Pr(X\vert C)\Pr(C)}{\sum_C\Pr(X\vert C)\Pr(C)}\\
\Pr(X\vert C) =& \Pr(X\vert C,\{m_i^C\})\\
=& \sum_h\Pr(X\vert C,h,m_h^C)\Pr(h\vert C,\{m_i^C\})\\
=& \sum_h\Pr(X\vert m_h^C)\Pr(h)\\
\Pr(X\vert m_h^C) =& \mathcal{N}(m_h^C,\mathbf{I}/5)\\
\Pr(h) =& \frac{1}{10}\\
\Pr(C) =& \frac{1}{2}
\end{align*}
So the boundary can be further simplified as
\[
\left\{x\biggm| \sum_{k=1}^{10} \mathcal{N}(m_k^\mathrm{Orange},\mathbf{I}/5)=\sum_{k=1}^{10} \mathcal{N}(m_k^\mathrm{Blue},\mathbf{I}/5)\right\}
\]
\end{sol}

%2.3
\begin{sol}

\end{sol}

