\section{Linear Methods for Regression}

%3.1
\begin{sol}
Let $\hat{\beta}$ be the parameter estimation for the bigger model and $\tilde{\beta}$ be the one for the smaller model. We first have
\begin{align*}
(\mathrm{RSS}_0-\mathrm{RSS}_1)/(p_1-p_0) = & \|y-X\tilde{\beta}\|^2 - \|y-X\hat{\beta}\|^2 \\
=& (y^\mathrm{T}y-2\tilde{\beta}^\mathrm{T}X^\mathrm{T}y+\tilde{\beta}^\mathrm{T}X^\mathrm{T}X\tilde{\beta}) - (y^\mathrm{T}y-2\hat{\beta}^\mathrm{T}X^\mathrm{T}y+\hat{\beta}^\mathrm{T}X^\mathrm{T}X\hat{\beta}) \\
=& 2(\hat{\beta}-\tilde{\beta})^\mathrm{T}X^\mathrm{T}(y-X\hat{\beta}) + (\hat{\beta}-\tilde{\beta})^\mathrm{T}X^\mathrm{T}X (\hat{\beta}-\tilde{\beta}) \\
=& 2(\hat{\beta}-\tilde{\beta})^\mathrm{T}(X^\mathrm{T}y-X^\mathrm{T}X(X^\mathrm{T}X)^{-1}X^\mathrm{T}y) + (\hat{\beta}-\tilde{\beta})^\mathrm{T}X^\mathrm{T}X (\hat{\beta}-\tilde{\beta}) \\
=& (\hat{\beta}-\tilde{\beta})^\mathrm{T}X^\mathrm{T}X (\hat{\beta}-\tilde{\beta})\\
=& \|X (\hat{\beta}-\tilde{\beta})\|^2
\end{align*}
Note that $\hat{\beta}$ and $\tilde{\beta}$ satisfy
\[
\hat{\beta}=\argmin_\beta\|y-X\beta\|^2
\]
and
\begin{align*}
\tilde{\beta}=&\argmin_\beta\|y-X\beta\|^2\\
\st &\beta_j=0
\end{align*}
Denote $e_j$ as the vector with the $j$-th element being 1 and others being 0. The optimality conditions are
\[
X^\mathrm{T}(y-X\hat{\beta})=0
\]
and
\[
\begin{cases}
& X^\mathrm{T}(y-X\tilde{\beta})-\lambda e_j=0\\
& \tilde{\beta}_j=0
\end{cases}
\]
which gives
\[
X^\mathrm{T}X (\hat{\beta}-\tilde{\beta}) = \lambda e_j \Longrightarrow \|X (\hat{\beta}-\tilde{\beta})\|^2 = \lambda^2 (X^\mathrm{T}X)^{-1}_{jj} = \lambda^2 (X^\mathrm{T}X)^{-1}_{jj}
\]
To determine the value of the Lagrange multiplier $\lambda$, one only needs to notice that
\begin{align*}
&\hat{\beta}-\tilde{\beta} = \lambda (X^\mathrm{T}X)^{-1} e_j = \lambda (X^\mathrm{T}X)^{-1}_j \\
\Longrightarrow & \hat{\beta}_j = \lambda (X^\mathrm{T}X)^{-1}_{jj} \\
\Longrightarrow & \lambda = \frac{\hat{\beta}_j}{(X^\mathrm{T}X)^{-1}_{jj}}
\end{align*}
where $(X^\mathrm{T}X)^{-1}_j$ is the $j$-th column of $(X^\mathrm{T}X)^{-1}$. If define $v_j\triangleq(X^\mathrm{T}X)^{-1}_{jj}$ and recall $\mathrm{RSS}_1/(N-p_1-1)=\hat{\sigma}^2$, we finally have
\[
F = \left(\frac{\hat{\beta}_j}{v_j}\right)^2 \frac{v_j}{\hat{\sigma}^2} = \frac{\hat{\beta}_j^2}{\hat{\sigma}^2v_j}
\]
\end{sol}

%3.2
\begin{sol}
Leave it later
\end{sol}

%3.3
\begin{sol}
\begin{enumerate}
\item Let $\theta=a^\mathrm{T}\beta$, $\hat{\theta}=a^\mathrm{T}\hat{\beta}=b^\mathrm{T}y$ and $\tilde{\theta}=c^\mathrm{T}y$, where $\hat{\beta}=(X^{\mathrm{T}}X)^{-1}X^{\mathrm{T}}y$, $b=X(X^{\mathrm{T}}X)^{-1}a$ and $y\sim\mathcal{N}(X\beta,\sigma^2I)$. From the unbiasedness of $\tilde{\theta}$, we know $\E[\tilde{\theta}]=c^{\mathrm{T}}X\beta=\E[\hat{\theta}]=a^\mathrm{T}\beta$. Expanding $\var[\hat{\theta}]$ and $\var[\tilde{\theta}]$ we will have
\begin{align*}
\var[\hat{\theta}] =& b^\mathrm{T}(\sigma^2 I)b\\
=& \sigma^2 a^\mathrm{T}(X^{\mathrm{T}}X)^{-1}X^\mathrm{T}X(X^{\mathrm{T}}X)^{-1}a\\
=& \sigma^2 a^\mathrm{T}(X^{\mathrm{T}}X)^{-1}a\\
\var[\tilde{\theta}] =& c^\mathrm{T}(\sigma^2 I)c\\
=& \sigma^2 c^\mathrm{T}c
\end{align*}
Now consider
\begin{align*}
\min_c \ & c^\mathrm{T}c\\
\st & c^{\mathrm{T}}X\beta=a^\mathrm{T}\beta
\end{align*}
The Lagrangian $L=c^\mathrm{T}c+\lambda(c^{\mathrm{T}}X\beta-a^\mathrm{T}\beta)$ gives the stationary condition:
\begin{align*}
\frac{\partial L}{\partial c}=& 2c+\lambda X\beta=0\\
\frac{\partial L}{\partial \lambda}=& c^{\mathrm{T}}X\beta-a^\mathrm{T}\beta=0
\end{align*}
Canceling the $\lambda$ gives
\[
c=\frac{a^\mathrm{T}\beta}{\beta^{\mathrm{T}}X^{\mathrm{T}}X\beta}\cdot X\beta
\]
So
\begin{align*}
& c^\mathrm{T}c\ge \frac{\left(a^\mathrm{T}\beta\right)^2}{\beta^{\mathrm{T}}X^{\mathrm{T}}X\beta}=\frac{\beta^{\mathrm{T}}aa^{\mathrm{T}}\beta}{\beta^{\mathrm{T}}X^{\mathrm{T}}X\beta}\\
\Longrightarrow & c^\mathrm{T}c\ge \max_\beta \frac{\beta^{\mathrm{T}}aa^{\mathrm{T}}\beta}{\beta^{\mathrm{T}}X^{\mathrm{T}}X\beta}
\end{align*}
The final optimization problem is equivalent to
\begin{align*}
\max_\beta \ & \beta^{\mathrm{T}}aa^{\mathrm{T}}\beta\\
\st & \beta^{\mathrm{T}}X^{\mathrm{T}}X\beta=1
\end{align*}
Again by Lagrange multiplier, we can see that the maximum is achieved at the eigenvector corresponding to the largest eigenvalue, denoted as $\mu$, of $A=aa^{\mathrm{T}}(X^{\mathrm{T}}X)^{-1}$. But since $\mathrm{rank}(A)=1$, so $\mu=\tr(A)=a^{\mathrm{T}}(X^{\mathrm{T}}X)^{-1}a$.
\item 
\end{enumerate}
\end{sol}
