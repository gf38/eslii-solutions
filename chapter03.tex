\section{Linear Methods for Regression}

%3.1
\begin{sol}
\label{prob:3-1}
Let $\hat{\beta}$ be the parameter estimation for the bigger model and $\tilde{\beta}$ be the one for the smaller model. We first have
\begin{align*}
(\mathrm{RSS}_0-\mathrm{RSS}_1)/(p_1-p_0) = & \|y-X\tilde{\beta}\|^2 - \|y-X\hat{\beta}\|^2 \\
=& (\Tr{y}y-2\Tr{\tilde{\beta}}\Tr{X}y+\Tr{\tilde{\beta}}\Tr{X}X\tilde{\beta}) - (\Tr{y}y-2\Tr{\hat{\beta}}\Tr{X}y+\Tr{\hat{\beta}}\Tr{X}X\hat{\beta}) \\
=& 2\Tr{(\hat{\beta}-\tilde{\beta})}\Tr{X}(y-X\hat{\beta}) + \Tr{(\hat{\beta}-\tilde{\beta})}\Tr{X}X (\hat{\beta}-\tilde{\beta}) \\
=& 2\Tr{(\hat{\beta}-\tilde{\beta})}(\Tr{X}y-\Tr{X}X(\Tr{X}X)^{-1}\Tr{X}y) + \Tr{(\hat{\beta}-\tilde{\beta})}\Tr{X}X (\hat{\beta}-\tilde{\beta}) \\
=& \Tr{(\hat{\beta}-\tilde{\beta})}\Tr{X}X (\hat{\beta}-\tilde{\beta})\\
=& \|X (\hat{\beta}-\tilde{\beta})\|^2
\end{align*}
Note that $\hat{\beta}$ and $\tilde{\beta}$ satisfy
\[
\hat{\beta}=\argmin_\beta\|y-X\beta\|^2
\]
and
\begin{align*}
\tilde{\beta}=&\argmin_\beta\|y-X\beta\|^2\\
\st &\beta_j=0
\end{align*}
Denote $e_j$ as the vector with the $j$-th element being 1 and others being 0. The optimality conditions are
\[
\Tr{X}(y-X\hat{\beta})=0
\]
and
\[
\begin{cases}
& \Tr{X}(y-X\tilde{\beta})-\lambda e_j=0\\
& \tilde{\beta}_j=0
\end{cases}
\]
which gives
\[
\Tr{X}X (\hat{\beta}-\tilde{\beta}) = \lambda e_j \Longrightarrow \|X (\hat{\beta}-\tilde{\beta})\|^2 = \lambda^2 (\Tr{X}X)^{-1}_{jj} = \lambda^2 (\Tr{X}X)^{-1}_{jj}
\]
To determine the value of the Lagrange multiplier $\lambda$, one only needs to notice that
\begin{align*}
&\hat{\beta}-\tilde{\beta} = \lambda (\Tr{X}X)^{-1} e_j = \lambda (\Tr{X}X)^{-1}_j \\
\Longrightarrow & \hat{\beta}_j = \lambda (\Tr{X}X)^{-1}_{jj} \\
\Longrightarrow & \lambda = \frac{\hat{\beta}_j}{(\Tr{X}X)^{-1}_{jj}}
\end{align*}
where $(\Tr{X}X)^{-1}_j$ is the $j$-th column of $(\Tr{X}X)^{-1}$. If define $v_j\triangleq(\Tr{X}X)^{-1}_{jj}$ and recall $\mathrm{RSS}_1/(N-p_1-1)=\hat{\sigma}^2$, we finally have
\[
F = \left(\frac{\hat{\beta}_j}{v_j}\right)^2 \frac{v_j}{\hat{\sigma}^2} = \frac{\hat{\beta}_j^2}{\hat{\sigma}^2v_j}
\]
\textbf{Update}: By the relationship between $t$-distribution and $F$-distribution
\begin{align*}
t_\nu=& \frac{Z}{\sqrt{\chi_\nu^2/\nu}}\\
=& \frac{\sqrt{\chi_1^2/1}}{\sqrt{\chi_\nu^2/\nu}}\\
=& \sqrt{F_{1,\nu}}
\end{align*}
one can finish the proof immediately.
\end{sol}

%3.2
\begin{sol}
The main difference happens in the order between taking projection and calculating confidence region.
\begin{enumerate}
\item This corresponds to first projecting $\beta$ onto $\alpha$ then calculating the confidence interval. We have
\begin{align*}
&\hat{\beta}\sim\mathcal{N}(\beta, \sigma^2(X^TX)^{-1})\\
\Longrightarrow & \Tr{\alpha}\hat{\beta}\sim\mathcal{N}(\Tr{\alpha}\beta, \sigma^2\Tr{\alpha}(X^TX)^{-1}\alpha)\\
\Longrightarrow & \frac{\Tr{\alpha}\hat{\beta} - \Tr{\alpha}\beta}{\hat{\sigma}\sqrt{\Tr{\alpha}(X^TX)^{-1}\alpha}} \sim t_{N-p-1}
\end{align*}
So the 95\% confidence band is
\[
\left[\Tr{\alpha}\hat{\beta}-\hat{\sigma}\sqrt{\Tr{\alpha}(X^TX)^{-1}\alpha}\cdot t_{N-p-1}^{(1-0.025)}, \Tr{\alpha}\hat{\beta}+\hat{\sigma}\sqrt{\Tr{\alpha}(X^TX)^{-1}\alpha}\cdot t_{N-p-1}^{(1-0.025)}\right]
\]
for each testing point $\alpha$.
\item This corresponds to first calculating the confidence region $C_\beta$ then projecting $\beta\in C_\beta$ onto $\alpha$. The lower boundary of the confidence interval is determined by
\begin{align*}
&\min_\beta \Tr{\alpha}\beta\\
\st& \Tr{(\beta-\hat{\beta})}X^TX(\beta-\hat{\beta})\le\hat{\sigma}^2{\chi_{p+1}^2}^{(1-0.05)}
\end{align*}
Or equivalently,
\begin{align*}
&\min_z \Tr{\alpha}z\\
\st& \Tr{z}X^TXz\le\hat{\sigma}^2{\chi_{p+1}^2}^{(1-0.05)}
\end{align*}
By KKT conditions, this becomes
\begin{align*}
\frac{\partial}{\partial z}\left\{\Tr{\alpha}z + \lambda\left[\Tr{z}X^TXz-\hat{\sigma}^2{\chi_{p+1}^2}^{(1-0.05)}\right]\right\}\bigg|_{z=z^*}=& 0\\
\lambda\left[z^{*\mathrm{T}}X^TXz^*-\hat{\sigma}^2{\chi_{p+1}^2}^{(1-0.05)}\right] =& 0
\end{align*}
which gives
\begin{align*}
\lambda=&\frac{1}{2}\sqrt{\frac{\Tr{\alpha}(X^TX)^{-1}\alpha}{\hat{\sigma}^2{\chi_{p+1}^2}^{1-0.95}}}\\
z^*=&\sqrt{\frac{\hat{\sigma}^2{\chi_{p+1}^2}^{1-0.95}}{\Tr{\alpha}(X^TX)^{-1}\alpha}}\cdot(X^TX)^{-1}\alpha
\end{align*}
It can be easily verified that $\lambda\ge0$ and $z^\mathrm{*T}X^TXz^*\le\hat{\sigma}^2{\chi_{p+1}^2}^{(1-0.05)}$, so they are feasible. This gives the confidence band
\[
\left[\Tr{\alpha}\hat{\beta}-\hat{\sigma}\sqrt{\Tr{\alpha}(X^TX)^{-1}\alpha\cdot {\chi_{p+1}^2}^{(1-0.05)}},
\Tr{\alpha}\hat{\beta}+\hat{\sigma}\sqrt{\Tr{\alpha}(X^TX)^{-1}\alpha\cdot {\chi_{p+1}^2}^{(1-0.05)}}
\right]
\]
\end{enumerate}
For a fixed $p$, $\sqrt{{\chi_{p+1}^2}^{(1-0.05)}}-t_{N-p-1}^{(1-0.025)}$ grows monotonically when $N$ grows, with the limit as $\sqrt{{\chi_{p+1}^2}^{(1-0.05)}}-\Phi(1-0.025)$, where $\Phi$ is the probit function. For $p=3$ and $N\ge 5$, $\sqrt{{\chi_{p+1}^2}^{(1-0.05)}}>t_{N-p-1}^{(1-0.025)}$ from $N\ge 8$. The limit of the difference is $\sqrt{{\chi_{4}^2}^{(1-0.05)}}-\Phi(1-0.025)\approx 1.120252$.
\end{sol}

%3.3
\begin{sol}
\begin{enumerate}
\item
\label{prob:3-3-1}
Let $\theta=\Tr{a}\beta$, $\hat{\theta}=\Tr{a}\hat{\beta}=\Tr{b}y$ and $\tilde{\theta}=\Tr{c}y$, where $\hat{\beta}=(\Tr{X}X)^{-1}\Tr{X}y$, $b=X(\Tr{X}X)^{-1}a$ and $y\sim\mathcal{N}(X\beta,\sigma^2I)$. From the unbiasedness of $\tilde{\theta}$, we know $\E[\tilde{\theta}]=\Tr{c}X\beta=\E[\hat{\theta}]=\Tr{a}\beta$. Expanding $\var[\hat{\theta}]$ and $\var[\tilde{\theta}]$ we will have
\begin{align*}
\var[\hat{\theta}] =& \Tr{b}(\sigma^2 I)b\\
=& \sigma^2 \Tr{a}(\Tr{X}X)^{-1}\Tr{X}X(\Tr{X}X)^{-1}a\\
=& \sigma^2 \Tr{a}(\Tr{X}X)^{-1}a\\
\var[\tilde{\theta}] =& \Tr{c}(\sigma^2 I)c\\
=& \sigma^2 \Tr{c}c
\end{align*}
Now consider
\begin{align*}
\min_c \ & \Tr{c}c\\
\st & \Tr{c}X\beta=\Tr{a}\beta
\end{align*}
The Lagrangian $L=\Tr{c}c+\lambda(\Tr{c}X\beta-\Tr{a}\beta)$ gives the stationary condition:
\begin{align*}
\frac{\partial L}{\partial c}=& 2c+\lambda X\beta=0\\
\frac{\partial L}{\partial \lambda}=& \Tr{c}X\beta-\Tr{a}\beta=0
\end{align*}
Canceling the $\lambda$ gives
\[
c=\frac{\Tr{a}\beta}{\Tr{\beta}\Tr{X}X\beta}\cdot X\beta
\]
So
\begin{align*}
& \Tr{c}c\ge \frac{\left(\Tr{a}\beta\right)^2}{\Tr{\beta}\Tr{X}X\beta}=\frac{\Tr{\beta}a\Tr{a}\beta}{\Tr{\beta}\Tr{X}X\beta}\\
\Longrightarrow & \Tr{c}c\ge \max_\beta \frac{\Tr{\beta}a\Tr{a}\beta}{\Tr{\beta}\Tr{X}X\beta}
\end{align*}
The final optimization problem is equivalent to
\begin{align*}
\max_\beta \ & \Tr{\beta}a\Tr{a}\beta\\
\st & \Tr{\beta}\Tr{X}X\beta=1
\end{align*}
Again by Lagrange multiplier, we can see that the maximum is the largest eigenvalue, denoted as $\mu$, of $A=a\Tr{a}(\Tr{X}X)^{-1}$. But since $\mathrm{rank}(A)=1$, so $\mu=\tr(A)=\Tr{a}(\Tr{X}X)^{-1}a$.
\item We similarly let $\tilde{\beta}=Cy\in\mathbb{R}^{p+1}$, then $\hat{V}=\sigma^2(\Tr{X}X)^{-1}$, $\tilde{V}=\sigma^2CC^{\mathrm{T}}$ and $\E[\tilde{\beta}]=CX\beta=\beta$. From \ref{prob:3-3-1}, $\forall a\in\mathbb{R}^{p+1}$, given $\Tr{a}CX\beta=\Tr{a}\beta$, we have
\begin{align*}
& \Tr{a}CC^{\mathrm{T}}a\ge \Tr{a}(\Tr{X}X)^{-1}a\\
\Longrightarrow & \Tr{a}\left(CC^{\mathrm{T}}-(\Tr{X}X)^{-1}\right)a\ge 0\\
\Longrightarrow & CC^{\mathrm{T}} - \left(\Tr{X}X\right)^{-1} \succeq 0\\
\Longrightarrow & CC^{\mathrm{T}}\succeq \left(\Tr{X}X\right)^{-1}
\end{align*}
\end{enumerate}
\end{sol}

%3.4
\begin{sol}
From $X=QR$ one can get $\hat{\beta}=(\Tr{X}X)^{-1}\Tr{X}y=R^{-1}\Tr{Q}y\Longrightarrow R\hat{\beta}=\Tr{Q}y$. Further we denote
\begin{align*}
X =& (x_1,\cdots,x_{p+1})\\
Q =& (e_1,\cdots,e_{p+1})=(\frac{z_1}{\|z_1\|},\cdots,\frac{z_{p+1}}{\|z_{p+1}\|})\\
r_{ij} =& \Tr{e_i}x_j
\end{align*}
Then we can write the equations involving individual $\hat{\beta}_i$ in a ``bottom-up'' way
\begin{align*}
(\Tr{e_{p+1}}x_{p+1})\hat{\beta}_{p+1} &= \Tr{e_{p+1}}y\\
(\Tr{e_{p}}x_{p})\hat{\beta}_{p} + (\Tr{e_{p}}x_{p+1})\hat{\beta}_{p+1} &= \Tr{e_{p}}y\\
&\vdotswithin{=}\\
(\Tr{e_{1}}x_{1})\hat{\beta}_{1} + \cdots + (\Tr{e_{1}}x_{p+1})\hat{\beta}_{p+1} &= \Tr{e_{1}}y
\end{align*}
This can be solved sequentially:
\begin{align*}
\hat{\beta}_{p+1} &= \frac{\Tr{e_{p+1}}y}{\Tr{e_{p+1}}x_{p+1}}\\
\hat{\beta}_{p} &= \frac{\Tr{e_{p}}y}{\Tr{e_{p}}x_{p}}-\frac{\Tr{e_{p}}x_{p+1}}{\Tr{e_{p}}x_{p}}\hat{\beta}_{p+1}\\
&\vdotswithin{=}\\
\hat{\beta}_{1} &= \frac{\Tr{e_{1}}y}{\Tr{e_{1}}x_{1}}- \frac{\Tr{e_{1}}x_{2}}{\Tr{e_{1}}x_{1}}\hat{\beta}_{2} - \cdots - \frac{\Tr{e_{1}}x_{p+1}}{\Tr{e_{1}}x_{1}}\hat{\beta}_{p+1}
\end{align*}
\end{sol}

%3.5
\begin{sol}
\begin{enumerate}
\item Ridge regression. From
\begin{align*}
\mathcal{L} =& \sum_{i=1}^{N}\left(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j\right)^2+\lambda\sum_{j=1}^{p}\beta_j^2\\
=& \sum_{i=1}^{N}\left[y_i-\left(\beta_0+\sum_{j=1}^{p}\bar{x}_j\beta_j\right)-\sum_{j=1}^{p}\left(x_{ij}-\bar{x}_j\right)\beta_j\right]^2+\lambda\sum_{j=1}^{p}\beta_j^2
\end{align*}
we can re-parameterize $\beta$ accordingly:
\[
\begin{cases}
\beta_0^c=\beta_0+\sum_{j=1}^{p}\bar{x}_j\beta_j\\
\beta_j^c=\beta_j\quad(1\le j\le p)
\end{cases}
\]
By setting $\partial \mathcal{L}/\partial \beta^c_0=0$, we have
\begin{align*}
& \sum_{i=1}^{N}\left[y_i-\beta_0^c-\sum_{j=1}^{p}\left(x_{ij}-\bar{x}_j\right)\beta^c_j\right]=0\\
\Longrightarrow & \beta_0^c=\frac{1}{N}\sum_{i=1}^{N}y_i-\frac{1}{N}\sum_{j=1}^{p}\sum_{i=1}^{N}\left(x_{ij}-\bar{x}_j\right)\beta^c_j=\frac{1}{N}\sum_{i=1}^{N}y_i
\end{align*}
Then one can follow the routine of ridge regression to get other $\beta_j^c$ with the centered design matrix.
\item Lasso. The regularization term does not affect the derivation of $\beta_0^c$, so we have the same $\beta_0^c$, with other $\beta_j^c$ obtained from any Lasso solver.
\end{enumerate}
\end{sol}

%3.6
\begin{sol}
From
\begin{align*}
P(\beta\vert y,X) \propto & P(y\vert\beta,X)P(\beta)\\
\propto& \exp\left[-\frac{1}{2\sigma^2}\left(\|y-X\beta\|^2+\frac{\sigma^2}{\tau}\|\beta\|^2\right)\right]
\end{align*}
we immediately know that $\lambda=\sigma^2/\tau$ and ridge regression estimate is the mode of the posterior distribution. To prove that it's also the mean, we let $\hat{\beta}$ be the ridge regression estimate:
\[
\hat{\beta}=\left(\Tr{X}X+\frac{\sigma^2}{\tau}I\right)^{-1}\Tr{X}y
\]
then
\begin{align*}
& \|y-X\beta\|^2+\frac{\sigma^2}{\tau}\|\beta\|^2 \\
=& \Tr{\beta}\left(\Tr{X}X+\frac{\sigma^2}{\tau}I\right)\beta - 2\Tr{y}X\beta + \Tr{y}y\\
=& \Tr{(\beta-\hat{\beta})}\left(\Tr{X}X+\frac{\sigma^2}{\tau}I\right)(\beta-\hat{\beta})-\Tr{\hat{\beta}}\left(\Tr{X}X+\frac{\sigma^2}{\tau}I\right)\hat{\beta}+\|y\|^2
\end{align*}
which can be minimized when $\beta=\hat{\beta}$. So $\hat{\beta}$ also maximizes the likelihood of the posterior distribution.
\end{sol}

%3.7
\begin{sol}
Similarly, the equivalence can be seen from
\begin{align*}
P(\beta_0,\beta\vert y,X) \propto & P(y\vert\beta_0,\beta,X)P(\beta)\\
=& \prod_i P(y_i\vert\beta_0,\beta,X)\cdot\prod_j P(\beta_j)\\
\propto& \prod_i \exp\left[-\frac{1}{2\sigma^2}\left(y_i-\beta_0-\Tr{x_i}\beta\right)\right] \cdot \prod_j \exp\left(-\frac{\beta_j^2}{2\tau^2}\right)\\
=& \exp\left\{-\frac{1}{2\sigma^2}\left[\sum_i \left(y_i-\beta_0-\sum_jx_{ij}\beta_j\right)+\frac{\sigma^2}{\tau^2}\sum_j \beta_j^2\right]\right\}
\end{align*}
\end{sol}

%3.8
\begin{sol}
Let $X_2$ be the feature matrix without the column of 1's. We first consider the case where $N>p$ and $X_2$ is not singular. 
From
\[
X=(e,X_2)=QR=\left(\frac{1}{\sqrt{n}}e,Q_2\right)
\begin{pmatrix}
\sqrt{n} & \frac{1}{\sqrt{n}}\Tr{e}X_2 \\
0 & R_2
\end{pmatrix}
=\left(e,\frac{1}{n}e\Tr{e}X_2+Q_2R_2\right)
\]
we know that 
\[
\tilde{X}=\left(I-\frac{1}{n}e\Tr{e}\right)X_2=Q_2R_2
\]
Meanwhile we have $\tilde{X}=U\Sigma \Tr{V}$, which gives $U=Q_2R_2(\Sigma \Tr{V})^{-1}$, implying $Q_2$ and $U$ span the same subspace. If $Q_2=U$ (up to sign flips, same below), then $R_2=\Tr{U}U\Sigma \Tr{V}=\Sigma \Tr{V}$, that is, rows of $R_2$ are orthogonal. But $R_2$ is upper triangle, so $R_2$ is diagonal, and subsequently the columns of $\tilde{X}$ are orthogonal.

In the case where $N\le p$ or $X$ is singular, $R_2$ and $V$ are not square matrices any more. One needs to replace the inverse with pseudo-inverse to get $\mathrm{span}(Q_2)=\mathrm{span}(U)$. But the condition for $Q_2=U$ is more difficult to find.
\end{sol}

%3.9
\begin{sol}
Let $X_1=QR$ and $Q=(z_1,\cdots,z_q)$. Suppose a new $\tilde{x}$ comes from the columns of $X_2$, we have
\[
(X_1, \tilde{x})=\tilde{Q}\tilde{R}=\left(Q,\frac{\tilde{x}-Q\Tr{Q}\tilde{x}}{\|\tilde{x}-Q\Tr{Q}\tilde{x}\|}\right)
\begin{pmatrix}
R & \Tr{Q}\tilde{x}\\
0 & \|\tilde{x}-Q\Tr{Q}\tilde{x}\|
\end{pmatrix}
\]
Denote $z_{q+1}=(\tilde{x}-Q\Tr{Q}\tilde{x})/\|\tilde{x}-Q\Tr{Q}\tilde{x}\|$, $\Omega=\mathrm{span}(Q)$, and $\tilde{\Omega}=\mathrm{span}(Q,z_{q+1})=\mathrm{span}(Q,\tilde{x})$, then it's easy to verify
$\Omega\subset\tilde{\Omega}$, $Q\Tr{Q}y\in \Omega$, $\tilde{Q}\Tr{\tilde{Q}}y\in \tilde{\Omega}$, $y-Q\Tr{Q}y\in \Omega^\perp$, and $y-\tilde{Q}\Tr{\tilde{Q}}y\in \tilde{\Omega}^\perp$. Define $\tilde{r}$ as the new residue, we have
\begin{align*}
r =& \|y-Q\Tr{Q}y\|^2\\
=& \|(y-\tilde{Q}\Tr{\tilde{Q}}y)+(\tilde{Q}\Tr{\tilde{Q}}y-Q\Tr{Q}y)\|^2\\
=& \tilde{r} + \|\tilde{Q}\Tr{\tilde{Q}}y-Q\Tr{Q}y\|^2 + 2\Tr{(y-\tilde{Q}\Tr{\tilde{Q}}y)}(\tilde{Q}\Tr{\tilde{Q}}y-Q\Tr{Q}y)\\
=& \tilde{r} + \|(\tilde{Q}\Tr{\tilde{Q}}-Q\Tr{Q})y\|^2\\
=& \tilde{r} + \left\|\left(\frac{\tilde{x}-Q\Tr{Q}\tilde{x}}{\|\tilde{x}-Q\Tr{Q}\tilde{x}\|}\right)\Tr{\left(\frac{\tilde{x}-Q\Tr{Q}\tilde{x}}{\|\tilde{x}-Q\Tr{Q}\tilde{x}\|}\right)}y\right\|^2\\
=& \tilde{r} + \left[\Tr{\tilde{x}}(I-Q\Tr{Q})y\right]^2
\end{align*}
So $\tilde{r}=r-[\Tr{\tilde{x}}(I-Q\Tr{Q})y]^2$. In practice, one can use this update rule to find the $\tilde{x}$ that gives the largest $\left[\Tr{\tilde{x}}(I-Q\Tr{Q})y\right]^2$.
\end{sol}

%3.10
\begin{sol}
As discussed in \thesection.\ref{prob:3-1}, just alternatingly drop the variable with the smallest Z-score and refit the model.
\end{sol}

%3.11
\begin{sol}
If $\Sigma_i=\Sigma$,
\begin{align*}
& \frac{\partial}{\partial B}\sum_{i=1}^N\Tr{\left(y_i-\Tr{B}x_i\right)}\Sigma^{-1}\left(y_i-\Tr{B}x_i\right)=0\\
\Longrightarrow & \sum_{i=1}^N\Sigma^{-1}\left(y_i-\Tr{B}x_i\right)\Tr{x_i}=0\\
\Longrightarrow & \Sigma^{-1}(\Tr{Y}-\Tr{B}\Tr{X})X=0\\
\Longrightarrow & B=(\Tr{X}X)^{-1}\Tr{X}Y
\end{align*}
If $\Sigma_i$ are different, let $\vect(\cdot)$ be the operator that ``column-ordered'' a matrix into a vector, then
\begin{align*}
& \sum_{i=1}^N\Sigma_i^{-1}\left(y_i-\Tr{B}x_i\right)\Tr{x_i}=0\\
\Longrightarrow & \sum_{i=1}^{N}x_i\Tr{x_i}B\Sigma_i^{-1}=\sum_{i=1}^{N}x_i\Tr{y_i}\Sigma_i^{-1} \\
\Longrightarrow & \left(\sum_{i=1}^{N}\Sigma_i^{-1}\otimes x_i\Tr{x_i}\right)\cdot\vect(B)=\vect\left(\sum_{i=1}^{N}x_i\Tr{y_i}\Sigma_i^{-1}\right)\\
\Longrightarrow & \vect(B)=\left(\sum_{i=1}^{N}\Sigma_i^{-1}\otimes x_i\Tr{x_i}\right)^{-1}\cdot\vect\left(\sum_{i=1}^{N}x_i\Tr{y_i}\Sigma_i^{-1}\right)
\end{align*}
In the last equality, if we set $\Sigma_i=\Sigma$, it becomes
\begin{align*}
\vect(B) =& \left(\Sigma^{-1}\otimes\sum_{i=1}^{N}x_i\Tr{x_i}\right)^{-1} \left(\Sigma^{-1}\otimes I\right)\cdot\vect\left(\sum_{i=1}^{N}x_i\Tr{y_i}\right)\\
=& \left[\Sigma\otimes\left(\sum_{i=1}^{N}x_i\Tr{x_i}\right)^{-1}\right] \left(\Sigma^{-1}\otimes I\right)\cdot\vect\left(\sum_{i=1}^{N}x_i\Tr{y_i}\right)\\
=& \left[\Sigma\otimes\left(\Tr{X}X\right)^{-1}\right] \left(\Sigma^{-1}\otimes I\right)\cdot\vect\left(\Tr{X}Y\right)\\
=& I\otimes \left(\Tr{X}X\right)^{-1}\cdot \vect\left(\Tr{X}Y\right)\\
=& \vect\left[\left(\Tr{X}X\right)^{-1}\Tr{X}Y\right]
\end{align*}
which goes back to
\[
B=\left(\Tr{X}X\right)^{-1}\Tr{X}Y
\]
\end{sol}

%3.12
\begin{sol}
Let
\begin{align*}
\tilde{X}=&\begin{pmatrix}
X\\
\sqrt{\lambda}I
\end{pmatrix}\\
\tilde{y}=&\begin{pmatrix}
y\\
0
\end{pmatrix}
\end{align*}
Direct verification shows that
\[
\hat{\tilde{\beta}}^{\mathrm{LS}}=\left(\Tr{\tilde{X}}\tilde{X}\right)^{-1}\Tr{\tilde{X}}\tilde{y}=\left(\Tr{X}X+\lambda I\right)^{-1}\Tr{X}y=\hat{\beta}^{\mathrm{ridge}}
\]
\end{sol}

%3.13
\begin{sol}
WLOG we suppose $X$ has been centered, so $\bar{y}=0$. Let $X=U\Sigma \Tr{V}$, with $V=(v_1,\cdots,v_M)$ invertible when $M=p$, and let $Z=(z_1,\cdots,z_M), \hat{\theta}=\Tr{(\theta_1,\cdots,\theta_M)}$. From $z_m=Xv_m$ we immediately have
\begin{align*}
&\hat{y}_{(M)}^\mathrm{pcr}=\sum_{m=1}^{M}\hat{\theta}_mz_m=X\sum_{m=1}^{M}\hat{\theta}_mv_m\\
\Longrightarrow & \hat{\beta}^\mathrm{pcr}(M)=\sum_{m=1}^{M}\hat{\theta}_mv_m
\end{align*}
On the other hand, rewriting $\hat{\theta}_m$ as $\hat{\theta}_m=(\Tr{z_m}z_m)^{-1}\Tr{z_m}y$ gives 
\begin{align*}
\hat{y}_{(p)}^\mathrm{pcr}=&\sum_{m=1}^{p}\hat{\theta}_mz_m\\
=& Z(\Tr{Z}Z)^{-1}\Tr{Z}y\\
=& XV\left[(XV)^{\mathrm{T}}(XV)\right]^{-1}\Tr{(XV)}y\\
=& X(\Tr{X}X)^{-1}\Tr{X}y
\end{align*}
so
\[
\hat{\beta}^\mathrm{pcr}(p)=\hat{\beta}^\mathrm{LS}
\]
\end{sol}

%3.14
\begin{sol}
Note that $\{x_j\}_{j=1}^p$ are now orthonormal: $\langle x_i, x_j\rangle=\delta_{ij}$. We have
\begin{align*}
\hat{\phi}_{2j} =& \langle x_j^{(1)}, y\rangle\\
=& \langle x_j-\frac{\langle z_1, x_j \rangle}{\langle z_1, z_1 \rangle} z_1,y \rangle\\
=& \langle x_j,y\rangle - \frac{\langle z_1, x_j \rangle\langle z_1, y \rangle}{\langle z_1, z_1 \rangle}\\
=& \hat{\phi}_{1j}-\frac{\hat{\phi}_{1j}\langle x_j,x_j \rangle\cdot\sum_{j=1}^p\hat{\phi}_{1j}\langle x_j,y\rangle}{\langle \sum_{j=1}^p\hat{\phi}_{1j}x_j,\sum_{j=1}^p\hat{\phi}_{1j}x_j\rangle}\\
=& \hat{\phi}_{1j}-\frac{\hat{\phi}_{1j}\cdot\sum_{j=1}^p\hat{\phi}_{1j}^2}{ \sum_{j=1}^p\hat{\phi}_{1j}^2}\\
=& 0
\end{align*}
\end{sol}

%3.15
\begin{sol}
When $m=0$,
\begin{align*}
\corr(y,X\alpha)^2\var(X\alpha)=& \frac{\Tr{y}X\alpha}{\|y\|^2}\\
\propto & \|\Tr{y}X\|^2\|\alpha\|^2\\
=& \|\Tr{y}X\|^2
\end{align*}
Two sides are equal iff $\alpha\propto \Tr{y}X$. So $\hat{\phi}_0$ solves the optimization problem. \\
TODO: $m\ge 1$.
\end{sol}

%3.16
\begin{sol}
When $X$ is orthonormal, $\Tr{X}X=I$ and $\hat{y}=\sum_i\hat{\beta}_ix_i\Rightarrow r^2=\|y-\hat{y}\|^2=\|y\|^2-\sum_i \hat{\beta}_i^2$.
\begin{enumerate}
	\item Best subset (size $M$). Let $X^{(M)}$ be the new design matrix after feature selection. Since $X^{(M)}$ is still orthonormal, so the only change in the residue calculation is the indices in the summands:
	\[
	r^2=\|y-\hat{y}^{(M)}\|^2=\|y\|^2-\sum_{i\in I} \hat{\beta}_i^2
	\]
	where $I$ with $|I|=M$ be the index set of the chosen features. To minimize the residue, we keep the $M$ largest $\hat{\beta}_i$ in the right hand side, which means
	\[
	\hat{\beta}_i^{(M)}=\hat{\beta}_i\cdot I(|\hat{\beta}_i|\ge |\hat{\beta}_{(M)}|)
	\]
	\item Ridge regression. Direct calculation shows
	\[
	\hat{\beta}^{\mathrm{Ridge}}=(\Tr{X}X+\lambda I)^{-1}\Tr{X}y=\hat{\beta}/(1+\lambda)
	\]
	\item Lasso. To optimize
	\[
	\min_\beta \frac{1}{2}\|y-X\beta\|_2^2+\lambda \|\beta\|_1,
	\] 
	we let $\bm{0}$ be in the subdifferential:
	\begin{align*}
	&\bm{0}\in-\Tr{X}(y-X\beta)+\lambda\cdot\partial\|\beta\|_1\\
	\Longrightarrow & 0\in -\hat{\beta}_i+\beta_i+\lambda\cdot\partial|\beta_i|\\
	\Longrightarrow & \beta_i\in\hat{\beta}_i-\lambda\cdot\partial|\beta_i|\\
	\Longrightarrow & \begin{cases}
	\beta_i=\hat{\beta}_i+\lambda & \mbox{if } \beta_i<0\\
	\beta_i=\hat{\beta}_i-\lambda & \mbox{if } \beta_i>0\\
	\beta_i\in[\hat{\beta}_i-\lambda, \hat{\beta}_i+\lambda] & \mbox{if } \beta_i=0
	\end{cases}\\
	\Longrightarrow & \begin{cases}
	\beta_i=\hat{\beta}_i+\lambda & \mbox{if } \hat{\beta}_i<-\lambda\\
	\beta_i=\hat{\beta}_i-\lambda & \mbox{if } \hat{\beta}_i>\lambda\\
	\beta_i=0 & \mbox{if }-\lambda\le\hat{\beta}_i\le\lambda
	\end{cases}\\
	\Longrightarrow & \hat{\beta}_i^{Lasso}=\sign(\hat{\beta_i})(\hat{\beta_i}-\lambda)_+
	\end{align*}
\end{enumerate}
\end{sol}

%3.17
\begin{sol}
Skipped.
\end{sol}

%3.18
\begin{sol}
	Later.
\end{sol}

%3.19
\begin{sol}
\begin{enumerate}
\item Ridge regression.
\begin{align*}
	\frac{\partial}{\partial \lambda}\|\hat{\beta}^{\mathrm{ridge}}\|^2 = & \frac{\partial}{\partial \lambda}\Tr{y}X(\Tr{X}X+\lambda I)^{-2}\Tr{X}y\\
	=& \Tr{y}X\left[\frac{\partial}{\partial \lambda} (\Tr{X}X+\lambda I)^{-2}\right]\Tr{X}y\\
	=& -2\Tr{y}X(\Tr{X}X+\lambda I)^{-3}\Tr{X}y\\
	\le & 0
\end{align*}
So when $\lambda\to 0$, $\|\hat{\beta}^{\mathrm{ridge}}\|_2$ increases.

\item Lasso. The following R code will create an artificial data set, where $\|\hat{\beta}^{\mathrm{Lasso}}\|_2$ may decrease when $\lambda\to 0$.
\begin{verbatim}
library(glmnet)
set.seed(0)
N <- 10
p <- 15
x1 <- rnorm(N)
X <- mat.or.vec(N, p)
X[, 1] <- x1
for (i in 2:p) {X[, i] <- x1 + rnorm(N, sd=0.2)}
beta <- rnorm(p, sd=10)
y <- X %*% beta + rnorm(N, sd=0.01)
model <- glmnet(X, y, family="gaussian", alpha=1, intercept=FALSE)
\end{verbatim}

Figure \ref{fig:3-18-profile} and \ref{fig:3-18-l2norm} show some behavior of the fitted Lasso model.\\

Notes:
\begin{enumerate}
	\item when $\lambda\to 0$, $\|\hat{\beta}^{\mathrm{Lasso}}\|_1$ increases. More generally, let $L(x;\lambda)=f(x)+\lambda g(x)$, and $x_i^*=\mathrm{arg\,min}_xL(x;\lambda_i)\,(i=1,2)$. Suppose $\lambda_2>\lambda_1$ and $g(x_2^*)>g(x_1^*)$, we have
	\begin{align*}
	&(\lambda_2-\lambda_1)(g(x_2^*)-g(x_1^*))>0\\
	\Longrightarrow & \lambda_1g(x_1^*)+\lambda_2g(x_2^*)>\lambda_1g(x_2^*)+\lambda_2g(x_1^*)\\
	\Longrightarrow & [f(x_1^*)+\lambda_1g(x_1^*)]+[f(x_2^*)+\lambda_2g(x_2^*)]\\
	&>[f(x_2^*)+\lambda_1g(x_2^*)]+[f(x_1^*)+\lambda_2g(x_1^*)] \\
	&\ge [f(x_1^*)+\lambda_1g(x_1^*)]+[f(x_2^*)+\lambda_2g(x_2^*)]
	\end{align*}
	which is a contradiction, so $g(x^*)$ doesn't decrease when $\lambda$ decreases. If $x_1^*\ne x_2^*$ and $L$ is strictly convex w.r.t. $x$ for any $\lambda$, $g(x^*)$ increases when $\lambda$ decreases. In the context of Lasso, $\|\hat{\beta}^{\mathrm{Lasso}}\|_1$ increases when $\lambda\to 0$.
	\item $\|\hat{\beta}^{\mathrm{Lasso}}\|_2$ will not decrease if each $\hat{\beta}^{\mathrm{Lasso}}_i$ changes monotonically when $\lambda$ changes. The Section 6 in \cite{hastie2007} mentions a sufficient and necessary condition for the monotonicity of $\beta^\mathrm{Lasso}$'s profile paths, and constructs an artificial data set violating the condition. 
\end{enumerate}

 

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/3_18_profile.tikz}
	\caption{The profile paths of the estimated $\hat{\beta}^{\mathrm{Lasso}}$. Only 5 variables are activated at most.}
	\label{fig:3-18-profile}
\end{figure}
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/3_18_l2norm.tikz}
	\caption{The relation between $\lambda$ and $\|\hat{\beta}^{\mathrm{Lasso}}\|_2$. For some interval of $\lambda$, $\|\hat{\beta}^{\mathrm{Lasso}}\|_2$ decreases as $\lambda$ decreases.}
	\label{fig:3-18-l2norm}
\end{figure}
\end{enumerate}
\end{sol}

